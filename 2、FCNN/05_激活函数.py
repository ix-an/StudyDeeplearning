import torch
import matplotlib.pyplot as plt

# è®¾ç½®ä¸­æ–‡æ˜¾ç¤º
plt.rcParams["font.sans-serif"] = ["SimHei"]
plt.rcParams["axes.unicode_minus"] = False


def plot_activation(func, func_name, x_range=(-10, 10), n_points=100):
    """ç»˜åˆ¶æ¿€æ´»å‡½æ•°åŠå…¶å¯¼æ•°"""
    # ç”Ÿæˆè¾“å…¥æ•°æ®
    x = torch.linspace(x_range[0], x_range[1], n_points)
    # è®¡ç®—å‡½æ•°å€¼
    y = func(x)

    # è®¡ç®—å¯¼æ•°ï¼ˆè‡ªåŠ¨æ±‚å¯¼ï¼‰
    x_grad = torch.linspace(x_range[0], x_range[1], n_points, requires_grad=True)
    y_grad = func(x_grad)
    y_grad.sum().backward()  # æ ‡é‡åå‘ä¼ æ’­
    grad = x_grad.grad.detach().numpy()

    # ç»˜å›¾
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
    # å‡½æ•°æ›²çº¿
    ax1.plot(x.numpy(), y.numpy())
    ax1.set_title(f"{func_name}å‡½æ•°")
    ax1.set_xlabel("x")
    ax1.set_ylabel("y")
    ax1.grid(True)
    # å¯¼æ•°æ›²çº¿
    ax2.plot(x_grad.detach().numpy(), grad, color="red")
    ax2.set_title(f"{func_name}å¯¼æ•°ï¼ˆğŸ“Œè€ƒç‚¹ï¼šåå‘ä¼ æ’­æ¢¯åº¦è®¡ç®—ï¼‰")
    ax2.set_xlabel("x")
    ax2.set_ylabel("æ¢¯åº¦")
    ax2.grid(True)
    plt.show()


# æµ‹è¯•ï¼šç»˜åˆ¶ReLUå’ŒSoftmaxï¼ˆç¤ºä¾‹ï¼‰
if __name__ == "__main__":
    # ReLUå‡½æ•°ï¼ˆğŸ“Œè€ƒç‚¹ï¼šè§£å†³æ¢¯åº¦æ¶ˆå¤±ï¼Œè®¡ç®—é«˜æ•ˆï¼‰
    plot_activation(torch.relu, "ReLU")


    # Softmaxå‡½æ•°ï¼ˆè¾“å…¥ä¸ºå‘é‡ï¼Œè¿™é‡Œç®€åŒ–ä¸ºå•å€¼æ¼”ç¤ºè¶‹åŠ¿ï¼‰
    def softmax_demo(x):
        return torch.softmax(x, dim=0)  # ğŸ“Œè€ƒç‚¹ï¼šdimå‚æ•°éœ€æŒ‡å®šï¼ˆé€šå¸¸ä¸º1ï¼ŒæŒ‰è¡Œè®¡ç®—ï¼‰


    plot_activation(softmax_demo, "Softmax", x_range=(-5, 5))